{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../dataset/rag_truth_train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"../dataset/rag_truth_dev.json\", \"r\") as f:\n",
    "    dev_data = json.load(f)\n",
    "with open(\"../dataset/rag_truth_test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(data):\n",
    "    for d in data:\n",
    "        d[\"text\"] = \"Please judge the following statement whether it includes hallucination or not based on the Document above: \" + d[\"text\"]\n",
    "    return data\n",
    "\n",
    "train_data = add_prefix(train_data)\n",
    "dev_data = add_prefix(dev_data)\n",
    "test_data = add_prefix(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_type: QA, Data2txt, Summary\n",
    "task_name = \"Data2txt\"\n",
    "train_data = [d for d in train_data if d[\"task_type\"] == task_name]\n",
    "dev_data = [d for d in dev_data if d[\"task_type\"] == task_name]\n",
    "test_data = [d for d in test_data if d[\"task_type\"] == task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def create_trip(data, id_list):\n",
    "    trip = []\n",
    "    for id in id_list:\n",
    "        num = 0\n",
    "        no_hal = []\n",
    "        has_hal = []\n",
    "        for d in data:\n",
    "            if d[\"source_id\"] == id:\n",
    "                num += 1\n",
    "                ref = d[\"ref\"]\n",
    "                if d[\"labels\"] == 0:\n",
    "                    no_hal.append(d[\"text\"])\n",
    "                else:\n",
    "                    has_hal.append(d[\"text\"])\n",
    "            if num == 6:\n",
    "                num = 0\n",
    "                if no_hal == [] or has_hal == []:\n",
    "                    break\n",
    "                random.seed(id)\n",
    "                no_hal = random.sample(no_hal, len(no_hal))\n",
    "                has_hal = random.sample(has_hal, len(has_hal))\n",
    "\n",
    "                if len(no_hal) == 1:\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": no_hal[0], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": has_hal[0], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[1], \"text\": has_hal[1], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[2], \"text\": has_hal[2], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[3], \"text\": has_hal[3], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[4], \"text\": has_hal[4], \"labels\": 1}\n",
    "                    )\n",
    "                elif len(no_hal) == 2:\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": no_hal[0], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": has_hal[0], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[1], \"text\": no_hal[1], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[1], \"text\": has_hal[1], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[2], \"text\": has_hal[2], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[3], \"text\": has_hal[3], \"labels\": 1}\n",
    "                    )\n",
    "                elif len(no_hal) == 3:\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": no_hal[0], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": has_hal[0], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[1], \"text\": no_hal[1], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[1], \"text\": has_hal[1], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[2], \"negative\": has_hal[2], \"text\": no_hal[2], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[2], \"negative\": has_hal[2], \"text\": has_hal[2], \"labels\": 1}\n",
    "                    )\n",
    "                elif len(no_hal) == 4:\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": no_hal[0], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": has_hal[0], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[1], \"text\": no_hal[1], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[1], \"text\": has_hal[1], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[2], \"negative\": has_hal[0], \"text\": no_hal[2], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[3], \"negative\": has_hal[1], \"text\": no_hal[3], \"labels\": 0}\n",
    "                    )\n",
    "                elif len(no_hal) == 5:\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": no_hal[0], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": has_hal[0], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[0], \"text\": no_hal[1], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[2], \"negative\": has_hal[0], \"text\": no_hal[2], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[3], \"negative\": has_hal[0], \"text\": no_hal[3], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[4], \"negative\": has_hal[0], \"text\": no_hal[4], \"labels\": 0}\n",
    "                    )\n",
    "\n",
    "                no_hal = []\n",
    "                has_hal = []\n",
    "                break\n",
    "\n",
    "    return trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2305 210 368\n"
     ]
    }
   ],
   "source": [
    "train_id = [d[\"source_id\"] for d in train_data]\n",
    "train_id = list(set(train_id))\n",
    "dev_id = [d[\"source_id\"] for d in dev_data]\n",
    "dev_id = list(set(dev_id))\n",
    "test_id = [d[\"source_id\"] for d in test_data]\n",
    "test_id = list(set(test_id))\n",
    "print(len(train_id), len(dev_id), len(test_id))\n",
    "train_trip = create_trip(train_data, train_id)\n",
    "dev_trip = create_trip(dev_data, dev_id)\n",
    "test_trip = create_trip(test_data, test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12336, 1116, 2208)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_trip), len(dev_trip), len(test_trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(train_trip)\n",
    "dev_df = pd.DataFrame(dev_trip)\n",
    "test_df = pd.DataFrame(test_trip)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "dev_ds = Dataset.from_pandas(dev_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "tri_raw_datasets = DatasetDict({\"train\": train_ds, \"dev\": dev_ds, \"test\": test_ds})\n",
    "tri_raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tri_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "tri_tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor': '{\\'name\\': \\'Madam Lu\\', \\'address\\': \\'3524 State St\\', \\'city\\': \\'Santa Barbara\\', \\'state\\': \\'CA\\', \\'categories\\': \\'Restaurants, Chinese\\', \\'hours\\': {\\'Monday\\': \\'11:0-21:0\\', \\'Tuesday\\': \\'11:0-21:0\\', \\'Wednesday\\': \\'11:0-21:0\\', \\'Thursday\\': \\'11:0-21:0\\', \\'Friday\\': \\'11:0-21:0\\', \\'Saturday\\': \\'11:0-21:0\\', \\'Sunday\\': \\'11:0-21:0\\'}, \\'attributes\\': {\\'BusinessParking\\': {\\'garage\\': False, \\'street\\': False, \\'validated\\': False, \\'lot\\': True, \\'valet\\': False}, \\'RestaurantsReservations\\': True, \\'OutdoorSeating\\': False, \\'WiFi\\': \\'no\\', \\'RestaurantsTakeOut\\': True, \\'RestaurantsGoodForGroups\\': True, \\'Music\\': None, \\'Ambience\\': {\\'romantic\\': False, \\'intimate\\': False, \\'touristy\\': False, \\'hipster\\': False, \\'divey\\': False, \\'classy\\': False, \\'trendy\\': False, \\'upscale\\': False, \\'casual\\': True}}, \\'business_stars\\': 3.5, \\'review_info\\': [{\\'review_stars\\': 5.0, \\'review_date\\': \\'2021-12-15 03:30:36\\', \\'review_text\\': \\'Friendly, fast, healthy, delicious. Not too salty or saucy. Saved the number in my phone.  Thanks! Yum!\\'}, {\\'review_stars\\': 5.0, \\'review_date\\': \\'2021-12-14 00:00:53\\', \\'review_text\\': \"I love madam Lu. Whoever the front ladies are that work there are super kind and welcoming. I usually order take out and enjoy seeing my family sharing madam Lu\\'s food. I personally really love the sizzling rice soup and imperial shrimp.\"}, {\\'review_stars\\': 5.0, \\'review_date\\': \\'2021-09-08 01:09:00\\', \\'review_text\\': \"I just had another wonderful dinner at Madame Lu\\'s. You can always count on fresh eggrolls and fresh vegetables and wonderful sauces on the lo mein dishes. You won\\'t be disappointed! She\\'s always happy to omit salt if you have a salt requirement. She\\'s also very accommodating when you have special needs or wishes. Give her a try!\"}]}',\n",
       " 'positive': 'Please judge the following statement whether it includes hallucination or not based on the Document above: Madam Lu is a popular Chinese restaurant located in Santa Barbara, CA. According to customer reviews, the restaurant is known for its friendly and fast service, as well as its healthy and delicious food. The menu offers a variety of options, including fresh egg rolls, fresh vegetables, and lo mein dishes made with wonderful sauces. The restaurant is also accommodating of special requests, such as omitting salt or accommodating dietary restrictions. Madam Lu has a casual atmosphere and offers takeout options, making it a great choice for a quick and easy meal. With an average rating of 3.5 stars and positive customer reviews, Madam Lu is a solid choice for those looking for good Chinese food in the area.',\n",
       " 'negative': 'Please judge the following statement whether it includes hallucination or not based on the Document above: Sure! Here is an objective overview of Madam Lu, a Chinese restaurant located in Santa Barbara, CA, based on the provided structured data:\\n\\nMadam Lu is a popular Chinese restaurant in Santa Barbara that offers a variety of traditional Chinese dishes with modern twists. The restaurant has a cozy and casual atmosphere, with no outdoor seating but validated parking available. The business is open seven days a week from 11am to 9pm, and accepts reservations and takes out orders. The menu includes a range of options such as sizzling rice soup, imperial shrimp, and lo mein dishes, all made with fresh ingredients and customizable to suit individual tastes. The front ladies who work there are known for their friendly and welcoming service. Customers rave about the freshness and quality of the food, as well as the accommodating nature of the staff. Overall, Madam Lu is a great choice for those looking for delicious and healthy Chinese cuisine in a cozy and casual setting.',\n",
       " 'text': 'Please judge the following statement whether it includes hallucination or not based on the Document above: Madam Lu is a popular Chinese restaurant located in Santa Barbara, CA. According to customer reviews, the restaurant is known for its friendly and fast service, as well as its healthy and delicious food. The menu offers a variety of options, including fresh egg rolls, fresh vegetables, and lo mein dishes made with wonderful sauces. The restaurant is also accommodating of special requests, such as omitting salt or accommodating dietary restrictions. Madam Lu has a casual atmosphere and offers takeout options, making it a great choice for a quick and easy meal. With an average rating of 3.5 stars and positive customer reviews, Madam Lu is a solid choice for those looking for good Chinese food in the area.',\n",
       " 'labels': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12336/12336 [00:05<00:00, 2119.49 examples/s]\n",
      "Map: 100%|██████████| 1116/1116 [00:00<00:00, 2037.66 examples/s]\n",
      "Map: 100%|██████████| 2208/2208 [00:01<00:00, 2048.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def tri_tokenize_function(examples):\n",
    "    anchor = tri_tokenizer(examples[\"anchor\"], truncation=True,max_length=512)\n",
    "    positive = tri_tokenizer(examples[\"positive\"], truncation=True,max_length=512)\n",
    "    negative = tri_tokenizer(examples[\"negative\"], truncation=True,max_length=512)\n",
    "    #text = tri_tokenizer(examples[\"text\"], truncation=True,max_length=512, padding=\"max_length\")\n",
    "\n",
    "    return {\n",
    "        \"anchor_input_ids\": anchor[\"input_ids\"],\n",
    "        \"anchor_attention_mask\": anchor[\"attention_mask\"],\n",
    "        \"positive_input_ids\": positive[\"input_ids\"],\n",
    "        \"positive_attention_mask\": positive[\"attention_mask\"],\n",
    "        \"negative_input_ids\": negative[\"input_ids\"],\n",
    "        \"negative_attention_mask\": negative[\"attention_mask\"],\n",
    "        #\"text_input_ids\": text[\"input_ids\"],\n",
    "        #\"text_attention_mask\": text[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "tri_tokenized_datasets = tri_raw_datasets.map(tri_tokenize_function, batched=True)\n",
    "tri_tokenized_datasets = tri_tokenized_datasets.remove_columns([\"anchor\", \"positive\", \"negative\", \"text\"])\n",
    "#tri_tokenized_datasets.set_format(\"torch\")\n",
    "tri_data_collator = DataCollatorWithPadding(tokenizer=tri_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "class CustomDataCollator(DataCollatorWithPadding):\n",
    "    def __call__(self, features):\n",
    "        # features: [{'anchor_input_ids': ..., 'anchor_attention_mask': ..., ...}, ...]\n",
    "        \n",
    "        anchor_ids = [torch.tensor(x['anchor_input_ids']).clone().detach() for x in features]\n",
    "        positive_ids = [torch.tensor(x['positive_input_ids']).clone().detach() for x in features]\n",
    "        negative_ids = [torch.tensor(x['negative_input_ids']).clone().detach() for x in features]\n",
    "        #text_ids = [torch.tensor(x['text_input_ids']).clone().detach() for x in features]\n",
    "        \n",
    "        anchor_mask = [torch.tensor(x['anchor_attention_mask']).clone().detach() for x in features]\n",
    "        positive_mask = [torch.tensor(x['positive_attention_mask']).clone().detach() for x in features]\n",
    "        negative_mask = [torch.tensor(x['negative_attention_mask']).clone().detach() for x in features]\n",
    "        #text_mask = [torch.tensor(x['text_attention_mask']).clone().detach() for x in features]\n",
    "\n",
    "        anchor_ids = pad_sequence(anchor_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        positive_ids = pad_sequence(positive_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        negative_ids = pad_sequence(negative_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        #text_ids = pad_sequence(text_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        \n",
    "        anchor_mask = pad_sequence(anchor_mask, batch_first=True, padding_value=0)\n",
    "        positive_mask = pad_sequence(positive_mask, batch_first=True, padding_value=0)\n",
    "        negative_mask = pad_sequence(negative_mask, batch_first=True, padding_value=0)\n",
    "        #text_mask = pad_sequence(text_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "        labels = torch.tensor([x['labels'] for x in features])\n",
    "        \n",
    "        batch = {\n",
    "            \"input_ids\": [anchor_ids, positive_ids, negative_ids],#, text_ids],\n",
    "            \"attention_mask\": [anchor_mask, positive_mask, negative_mask],#, text_mask],\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "\n",
    "\n",
    "tri_data_collator = CustomDataCollator(tokenizer=tri_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "base_model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "def triplet_loss(anchor_output, positive_output, negative_output, text_logits, labels):\n",
    "    text_targets = torch.tensor(labels).clone().detach().to(device)\n",
    "    classification_loss = nn.CrossEntropyLoss()(text_logits, text_targets)\n",
    "\n",
    "    triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "        margin=0.5, distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)\n",
    "    )\n",
    "    triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "    return classification_loss, triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    logits = logits[0]\n",
    "    predictions = np.argmax(logits, axis=-1).tolist()\n",
    "\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"recall\": recall, \"precision\": precision, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from models.models_phi import TripletModel\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",  \n",
    "    save_steps=10000,\n",
    "    learning_rate=1e-6,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    fp16 = True,\n",
    "    gradient_accumulation_steps=12,\n",
    "    logging_dir=\"./logs\",\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[],\n",
    "    optim=\"adafactor\",\n",
    ")\n",
    "\n",
    "tri_model = TripletModel(base_model, triplet_loss)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=tri_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tri_tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tri_tokenized_datasets[\"dev\"],\n",
    "    data_collator=tri_data_collator,\n",
    "    tokenizer=tri_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=tri_tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import os\n",
    "\n",
    "name = \"../trained_model/triplet_phi\"\n",
    "trainer.save_model(name)\n",
    "trainer.save_state()\n",
    "tri_model.save_pretrained(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you restart kernel, you need to load the model and tokenizer again.\n",
    "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "from models.models_phi import TripletModel\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def triplet_loss(anchor_output, positive_output, negative_output, text_logits, labels):\n",
    "    text_targets = torch.tensor(labels).clone().detach().to(device)\n",
    "    classification_loss = nn.CrossEntropyLoss()(text_logits, text_targets)\n",
    "\n",
    "    triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "        margin=0.5, distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)\n",
    "    )\n",
    "    triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "    return classification_loss, triplet_loss\n",
    "\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "# load model and tokenizer\n",
    "name = \"../trained_model/triplet_phi\"\n",
    "tri_model = TripletModel.from_pretrained(base_model, triplet_loss, name)\n",
    "tri_tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "tri_model.to(device)\n",
    "\n",
    "\n",
    "def tri_tokenize_function(examples):\n",
    "    anchor = tri_tokenizer(examples[\"anchor\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    positive = tri_tokenizer(examples[\"positive\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    negative = tri_tokenizer(examples[\"negative\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    #text = tri_tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "    return {\n",
    "        \"anchor_input_ids\": anchor[\"input_ids\"],\n",
    "        \"anchor_attention_mask\": anchor[\"attention_mask\"],\n",
    "        \"positive_input_ids\": positive[\"input_ids\"],\n",
    "        \"positive_attention_mask\": positive[\"attention_mask\"],\n",
    "        \"negative_input_ids\": negative[\"input_ids\"],\n",
    "        \"negative_attention_mask\": negative[\"attention_mask\"],\n",
    "        #\"text_input_ids\": text[\"input_ids\"],\n",
    "        #\"text_attention_mask\": text[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "\n",
    "tri_tokenized_datasets = tri_raw_datasets.map(tri_tokenize_function, batched=True)\n",
    "tri_tokenized_datasets = tri_tokenized_datasets.remove_columns([\"anchor\", \"positive\", \"negative\", \"text\"])\n",
    "# tokenized_datasets.set_format(\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tri_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"../results_test.json\"):\n",
    "    with open(\"../results_test.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    data = []\n",
    "    for i, d in enumerate(test_data):\n",
    "        data.append({\"id\": i, \"label\": d[\"labels\"], \"task\": d[\"task_type\"]})\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "tri_model.eval()\n",
    "\n",
    "\n",
    "for i,d in tqdm(enumerate(tri_tokenized_datasets[\"test\"])):\n",
    "    anchor_input_ids = torch.tensor(d[\"anchor_input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    positive_input_ids = torch.tensor(d[\"positive_input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    negative_input_ids = torch.tensor(d[\"negative_input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    input_ids = [anchor_input_ids, positive_input_ids, negative_input_ids]\n",
    "    \n",
    "    anchor_attention_mask = torch.tensor(d[\"anchor_attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    positive_attention_mask = torch.tensor(d[\"positive_attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    negative_attention_mask = torch.tensor(d[\"negative_attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask = [anchor_attention_mask, positive_attention_mask, negative_attention_mask]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = tri_model(input_ids=input_ids, attention_mask=attention_mask, labels=[d[\"labels\"]])\n",
    "        logits = outputs.logits\n",
    "        predicted_index = torch.argmax(logits, dim=-1)\n",
    "    data[i][\"triplet_phi_logits\"] = logits.cpu().numpy().tolist()[0]\n",
    "    data[i][\"triplet_phi_label\"] = predicted_index.cpu().numpy().tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "true_labels = [item['label'] for item in data]\n",
    "predicted_labels = [item['triplet_phi_label'] for item in data]\n",
    "\n",
    "count = Counter(predicted_labels)\n",
    "print(count)\n",
    "\n",
    "accuracy_score(true_labels, predicted_labels), f1_score(true_labels, predicted_labels), precision_score(true_labels, predicted_labels), recall_score(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results_test.json\", \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".acl2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
