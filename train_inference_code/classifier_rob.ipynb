{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../dataset/rag_truth_train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"../dataset/rag_truth_dev.json\", \"r\") as f:\n",
    "    dev_data = json.load(f)\n",
    "with open(\"../dataset/rag_truth_test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(data):\n",
    "    for d in data:\n",
    "        d[\"text\"] = \"Please judge the following statement whether it includes hallucination or not based on the Document above: \" + d[\"text\"]\n",
    "    return data\n",
    "\n",
    "train_data = add_prefix(train_data)\n",
    "dev_data = add_prefix(dev_data)\n",
    "test_data = add_prefix(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to train on specific task, please run the following code.\n",
    "# task_type: QA, Data2txt, Summary\n",
    "task_name = \"Summary\"\n",
    "train_data = [d for d in train_data if d[\"task_type\"] != task_name]\n",
    "dev_data = [d for d in dev_data if d[\"task_type\"] != task_name]\n",
    "test_data = [d for d in test_data if d[\"task_type\"] == task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "dev_df = pd.DataFrame(dev_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "dev_ds = Dataset.from_pandas(dev_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "raw_datasets = DatasetDict({\"train\": train_ds, \"dev\":dev_ds, \"test\": test_ds})\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    ref = tokenizer(examples[\"ref\"],truncation=True, max_length=512,padding=\"max_length\")\n",
    "    text = tokenizer(examples[\"text\"],truncation=True, max_length=512,padding=\"max_length\")\n",
    "    return {\n",
    "        \"ref_input_ids\":ref[\"input_ids\"],\n",
    "        \"ref_attention_mask\":ref[\"attention_mask\"],\n",
    "        \"text_input_ids\":text[\"input_ids\"],\n",
    "        \"text_attention_mask\":text[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\",\"ref\"])\n",
    "#tokenized_datasets.set_format(\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "class CustomDataCollator(DataCollatorWithPadding):\n",
    "    def __call__(self, features):\n",
    "        # features : [{'anchor_input_ids': ..., 'anchor_attention_mask': ..., ...}, ...]\n",
    "        \n",
    "        ref_ids = [torch.tensor(x['ref_input_ids']) for x in features]\n",
    "        text_ids = [torch.tensor(x['text_input_ids']) for x in features]\n",
    "       \n",
    "        \n",
    "        ref_mask = [torch.tensor(x['ref_attention_mask']) for x in features]\n",
    "        text_mask = [torch.tensor(x['text_attention_mask']) for x in features]\n",
    "        labels = torch.tensor([x['labels'] for x in features])\n",
    "        \n",
    "        ref_ids = pad_sequence(ref_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        text_ids = pad_sequence(text_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        \n",
    "        ref_mask = pad_sequence(ref_mask, batch_first=True, padding_value=0)\n",
    "        text_mask = pad_sequence(text_mask, batch_first=True, padding_value=0)\n",
    "        \n",
    "        batch = {\n",
    "            \"input_ids\": [ref_ids, text_ids],\n",
    "            \"attention_mask\": [ref_mask, text_mask],\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = CustomDataCollator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'source', 'model', 'task_type', 'source_id', 'ref_input_ids', 'ref_attention_mask', 'text_input_ids', 'text_attention_mask'],\n",
       "        num_rows: 13830\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['labels', 'source', 'model', 'task_type', 'source_id', 'ref_input_ids', 'ref_attention_mask', 'text_input_ids', 'text_attention_mask'],\n",
       "        num_rows: 1260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'source', 'model', 'task_type', 'source_id', 'ref_input_ids', 'ref_attention_mask', 'text_input_ids', 'text_attention_mask'],\n",
       "        num_rows: 2208\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"FacebookAI/RoBERTa-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "base_model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1).tolist() \n",
    "    labels = labels.tolist() \n",
    "  \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from models.models_rob import ClassifierModel\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    report_to = [],\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",  \n",
    "    save_steps=10000,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    fp16 = True,\n",
    "    gradient_accumulation_steps=12,\n",
    "    logging_dir=\"./logs\",\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"adafactor\",\n",
    ")\n",
    "\n",
    "model = ClassifierModel(base_model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"dev\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import os\n",
    "\n",
    "name = \"../trained_model/classifier_rob\"\n",
    "trainer.save_model(name)\n",
    "trainer.save_state()\n",
    "model.save_pretrained(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ref': 'The FBI charged a Philadelphia woman on Thursday with trying to travel overseas to fight for ISIS. She\\'s one of three women arrested this week on terror charges. Two New York women were also taken into custody. An FBI complaint cites numerous social media messages dating back to August 2013 that were sent by Keonna Thomas, 30, also known as \"Young Lioness\" and \"Fatayat Al Khilafah.\" One Twitter message said, \"If we truly knew the realities ... we all would be rushing to join our brothers in the front lines pray ALLAH accept us as shuhada [martyrs].\" Another said, \"When you\\'re a mujahid [violent jihadi fighter] your death becomes a wedding.\" The FBI said Thomas purchased an electronic visa to Turkey on March 23. Turkey is known as the easiest place from which to enter Syria and join ISIS. An ISIS manual advises recruits to buy round-trip tickets to vacation spots such as Spain and then purchase tickets for their real destination once they arrive overseas, the FBI said. On March 26, Thomas purchased a ticket to Barcelona, with a March 29 departure and an April 15 return to the United States, the complaint said. It\\'s not clear when or where she was arrested. She was charged with knowingly attempting to provide material support and resources to a designated foreign terrorist organization. She could be sentenced to 15 years in prison. On Thursday, Noelle Velentzas, 28, and her former roommate, Asia Siddiqui, 31, were arrested in New York and accused of planning to build an explosive device for attacks in the United States, federal prosecutors said. In the past 18 months, the Justice Department\\'s National Security Division has prosecuted or is prosecuting more than 30 cases of people attempting to travel abroad to join or provide support to terrorist groups. Of those cases, 18 allegedly involve support to ISIS. \"The terrorist threat is more decentralized, more diffuse, more complicated,\" Homeland Security Secretary Jeh Johnson told reporters Thursday. \"It involves the potential lone wolf actor, it involves the effective use of social media, the Internet.\"\\n',\n",
       " 'text': 'Please judge the following statement whether it includes hallucination or not based on the Document above: The FBI has charged a Philadelphia woman, Keonna Thomas, with trying to travel overseas to fight for ISIS. This follows the arrests of two New York women, Noelle Velentzas and Asia Siddiqui, who were accused of planning to build an explosive device for attacks in the United States. Thomas purchased an electronic visa to Turkey and a round-trip ticket to Barcelona, following the advice given in an ISIS manual for recruits. She has been charged with attempting to provide material support to a terrorist organisation and could face 15 years in prison.',\n",
       " 'labels': 0,\n",
       " 'source': 'CNN/DM',\n",
       " 'model': 'gpt-4-0613',\n",
       " 'task_type': 'Summary',\n",
       " 'source_id': '15596'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you restart kernel, you need to load the model and tokenizer again.\n",
    "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "from models.models_rob import ClassifierModel\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "# load model and tokenizer\n",
    "name = \"../trained_models/classifier_rob\"\n",
    "model = ClassifierModel.from_pretrained(base_model,name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    ref = tokenizer(examples[\"ref\"],truncation=True, max_length=512)\n",
    "    text = tokenizer(examples[\"text\"],truncation=True, max_length=512)\n",
    "    return {\n",
    "        \"ref_input_ids\":ref[\"input_ids\"],\n",
    "        \"ref_attention_mask\":ref[\"attention_mask\"],\n",
    "        \"text_input_ids\":text[\"input_ids\"],\n",
    "        \"text_attention_mask\":text[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\",\"ref\"])\n",
    "#tokenized_datasets.set_format(\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"../results_test.json\"):\n",
    "    with open(\"../results_test.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    data = []\n",
    "    for i, d in enumerate(test_data):\n",
    "        data.append({\"id\": i, \"label\": d[\"labels\"], \"task\": d[\"task_type\"]})\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for i,d in tqdm(enumerate(tokenized_datasets[\"test\"])):\n",
    "    ref_input_ids = torch.tensor(d[\"ref_input_ids\"]).unsqueeze(0).to(device)\n",
    "    text_input_ids = torch.tensor(d[\"text_input_ids\"]).unsqueeze(0).to(device)\n",
    "    input_ids = [ref_input_ids, text_input_ids]\n",
    "    ref_attention_mask = torch.tensor(d[\"ref_attention_mask\"]).unsqueeze(0).to(device)\n",
    "    text_attention_mask = torch.tensor(d[\"text_attention_mask\"]).unsqueeze(0).to(device)\n",
    "    attention_mask = [ref_attention_mask, text_attention_mask]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[\"logits\"]\n",
    "        predicted_index = torch.argmax(logits, dim=-1)\n",
    "    data[i][\"cls_rob_logits\"] = logits.cpu().numpy().tolist()[0]\n",
    "    data[i][\"cls_rob_label\"] = predicted_index.cpu().numpy().tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "true_labels = [item['label'] for item in data]\n",
    "predicted_labels = [item['cls_rob_label'] for item in data]\n",
    "\n",
    "count = Counter(predicted_labels)\n",
    "print(count)\n",
    "\n",
    "accuracy_score(true_labels, predicted_labels), f1_score(true_labels, predicted_labels), precision_score(true_labels, predicted_labels), recall_score(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results_test.json\", \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".acl2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
