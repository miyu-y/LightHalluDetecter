{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../dataset/rag_truth_train.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"../dataset/rag_truth_dev.json\", \"r\") as f:\n",
    "    dev_data = json.load(f)\n",
    "with open(\"../dataset/rag_truth_test.json\", \"r\") as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(data):\n",
    "    for d in data:\n",
    "        d[\"text\"] = (\n",
    "            \"Please judge the following statement whether it includes hallucination or not based on the Document above: \"\n",
    "            + d[\"text\"]\n",
    "        )\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = add_prefix(train_data)\n",
    "dev_data = add_prefix(dev_data)\n",
    "test_data = add_prefix(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_type: QA, Data2txt, Summary\n",
    "task_name = \"Summary\"\n",
    "train_data = [d for d in train_data if d[\"task_type\"] == task_name]\n",
    "dev_data = [d for d in dev_data if d[\"task_type\"] == task_name]\n",
    "test_data = [d for d in test_data if d[\"task_type\"] == task_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def create_trip(data, id_list):\n",
    "    trip = []\n",
    "    for id in id_list:\n",
    "        num = 0\n",
    "        no_hal = []\n",
    "        has_hal = []\n",
    "        for d in data:\n",
    "            if d[\"source_id\"] == id:\n",
    "                num += 1\n",
    "                ref = d[\"ref\"]\n",
    "                if d[\"labels\"] == 0:\n",
    "                    no_hal.append(d[\"text\"])\n",
    "                else:\n",
    "                    has_hal.append(d[\"text\"])\n",
    "            if num == 6:\n",
    "                num = 0\n",
    "                if no_hal == [] or has_hal == []:\n",
    "                    break\n",
    "                random.seed(id)\n",
    "                no_hal = random.sample(no_hal, len(no_hal))\n",
    "                has_hal = random.sample(has_hal, len(has_hal))\n",
    "\n",
    "                if len(no_hal) == 1:\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": no_hal[0], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": has_hal[0], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[1], \"text\": has_hal[1], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[2], \"text\": has_hal[2], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[3], \"text\": has_hal[3], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[4], \"text\": has_hal[4], \"labels\": 1}\n",
    "                    )\n",
    "                elif len(no_hal) == 2:\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": no_hal[0], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": has_hal[0], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[1], \"text\": no_hal[1], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[1], \"text\": has_hal[1], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[2], \"text\": has_hal[2], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[3], \"text\": has_hal[3], \"labels\": 1}\n",
    "                    )\n",
    "                elif len(no_hal) == 3:\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": no_hal[0], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": has_hal[0], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[1], \"text\": no_hal[1], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[1], \"text\": has_hal[1], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[2], \"negative\": has_hal[2], \"text\": no_hal[2], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[2], \"negative\": has_hal[2], \"text\": has_hal[2], \"labels\": 1}\n",
    "                    )\n",
    "                elif len(no_hal) == 4:\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": no_hal[0], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": has_hal[0], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[1], \"text\": no_hal[1], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[1], \"text\": has_hal[1], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[2], \"negative\": has_hal[0], \"text\": no_hal[2], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[3], \"negative\": has_hal[1], \"text\": no_hal[3], \"labels\": 0}\n",
    "                    )\n",
    "                elif len(no_hal) == 5:\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": no_hal[0], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[0], \"negative\": has_hal[0], \"text\": has_hal[0], \"labels\": 1}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[1], \"negative\": has_hal[0], \"text\": no_hal[1], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[2], \"negative\": has_hal[0], \"text\": no_hal[2], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[3], \"negative\": has_hal[0], \"text\": no_hal[3], \"labels\": 0}\n",
    "                    )\n",
    "                    trip.append(\n",
    "                        {\"anchor\": ref, \"positive\": no_hal[4], \"negative\": has_hal[0], \"text\": no_hal[4], \"labels\": 0}\n",
    "                    )\n",
    "\n",
    "                no_hal = []\n",
    "                has_hal = []\n",
    "                break\n",
    "\n",
    "    return trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2305 210 368\n"
     ]
    }
   ],
   "source": [
    "train_id = [d[\"source_id\"] for d in train_data]\n",
    "train_id = list(set(train_id))\n",
    "dev_id = [d[\"source_id\"] for d in dev_data]\n",
    "dev_id = list(set(dev_id))\n",
    "test_id = [d[\"source_id\"] for d in test_data]\n",
    "test_id = list(set(test_id))\n",
    "print(len(train_id), len(dev_id), len(test_id))\n",
    "train_trip = create_trip(train_data, train_id)\n",
    "dev_trip = create_trip(dev_data, dev_id)\n",
    "test_trip = create_trip(test_data, test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12336, 1116, 2208)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_trip), len(dev_trip), len(test_trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(train_trip)\n",
    "dev_df = pd.DataFrame(dev_trip)\n",
    "test_df = pd.DataFrame(test_trip)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "dev_ds = Dataset.from_pandas(dev_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "tri_raw_datasets = DatasetDict({\"train\": train_ds, \"dev\": dev_ds, \"test\": test_ds})\n",
    "tri_raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tri_tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/RoBERTa-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor': \"{'question': 'how to cure a swollen foot', 'passages': 'passage 1:Adding a healthy dose of Epsom salts to the tub is another way to boost that bath. Like the grapefruit oil, you can also just use a bucket, if you’re only having swelling in your feet and ankles. If your calves are swollen, too, a bath is probably your best bet. Epsom salts are like magic for swelling.wimming or even floating in water can help with swollen feet and ankles. The pressure from the water can help get things moving in your legs, and floating gives your circulatory system a break from gravity’s constant pull. If you don’t have a pool, check out local gyms to see what they offer. 10.\\\\n\\\\npassage 2:Check out some of these natural solutions to help relieve swollen feet and ankles! Swelling in your feet and ankles is called edema, and there are a lot of things that can cause the problem, from high blood pressure to pregnancy.wimming or even floating in water can help with swollen feet and ankles. The pressure from the water can help get things moving in your legs, and floating gives your circulatory system a break from gravity’s constant pull. If you don’t have a pool, check out local gyms to see what they offer. 10.\\\\n\\\\npassage 3:Swollen ankles and feet. Swollen ankles and swollen feet are common and are often caused by fluid retention, or oedema. The cause of the swelling can range from an injury to medical conditions. Seek medical advice if you are concerned about swollen feet or ankles.o reduce the swelling from a foot or ankle injury, rest to avoid walking on the injured ankle or foot. Use an ice pack wrapped in a towel (don’t use in direct contact with the skin) for up to 20 minutes, reapply if necessary after a 20 minute break.\\\\n\\\\n'}\",\n",
       " 'positive': 'Please judge the following statement whether it includes hallucination or not based on the Document above: Based on the given passages, adding Epsom salts to a bath or swimming or floating in water can help with swollen feet and ankles by reducing the swelling. Additionally, resting the injured ankle or foot and using an ice pack can also help with swelling. It is important to note that seeking medical advice is recommended if concerned about swollen feet or ankles.',\n",
       " 'negative': \"Please judge the following statement whether it includes hallucination or not based on the Document above: Based on the given passages, here are some ways to cure a swollen foot:\\n\\n1. Add Epsom salt to your bath water to reduce swelling.\\n2. Soak your foot in warm water for 20 minutes to improve circulation and reduce swelling.\\n3. Rest the injured foot or ankle to avoid aggravating the condition.\\n4. Apply an ice pack wrapped in a towel to the affected area for up to 20 minutes at a time, taking breaks as needed.\\n\\nIt's important to note that if the swelling is severe or persistent, you should seek medical advice from a healthcare professional for proper evaluation and treatment.\",\n",
       " 'text': 'Please judge the following statement whether it includes hallucination or not based on the Document above: Based on the given passages, adding Epsom salts to a bath or swimming or floating in water can help with swollen feet and ankles by reducing the swelling. Additionally, resting the injured ankle or foot and using an ice pack can also help with swelling. It is important to note that seeking medical advice is recommended if concerned about swollen feet or ankles.',\n",
       " 'labels': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "def tri_tokenize_function(examples):\n",
    "    anchor = tri_tokenizer(examples[\"anchor\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    positive = tri_tokenizer(examples[\"positive\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    negative = tri_tokenizer(examples[\"negative\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    #text = tri_tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "    return {\n",
    "        \"anchor_input_ids\": anchor[\"input_ids\"],\n",
    "        \"anchor_attention_mask\": anchor[\"attention_mask\"],\n",
    "        \"positive_input_ids\": positive[\"input_ids\"],\n",
    "        \"positive_attention_mask\": positive[\"attention_mask\"],\n",
    "        \"negative_input_ids\": negative[\"input_ids\"],\n",
    "        \"negative_attention_mask\": negative[\"attention_mask\"],\n",
    "        #\"text_input_ids\": text[\"input_ids\"],\n",
    "        #\"text_attention_mask\": text[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "\n",
    "tri_tokenized_datasets = tri_raw_datasets.map(tri_tokenize_function, batched=True)\n",
    "tri_tokenized_datasets = tri_tokenized_datasets.remove_columns([\"anchor\", \"positive\", \"negative\", \"text\"])\n",
    "# tri_tokenized_datasets.set_format(\"torch\")\n",
    "tri_data_collator = DataCollatorWithPadding(tokenizer=tri_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "\n",
    "class CustomDataCollator(DataCollatorWithPadding):\n",
    "    def __call__(self, features):\n",
    "        # features: [{'anchor_input_ids': ..., 'anchor_attention_mask': ..., ...}, ...]\n",
    "\n",
    "        anchor_ids = [torch.tensor(x[\"anchor_input_ids\"]).clone().detach() for x in features]\n",
    "        positive_ids = [torch.tensor(x[\"positive_input_ids\"]).clone().detach() for x in features]\n",
    "        negative_ids = [torch.tensor(x[\"negative_input_ids\"]).clone().detach() for x in features]\n",
    "        #text_ids = [torch.tensor(x[\"text_input_ids\"]).clone().detach() for x in features]\n",
    "\n",
    "        anchor_mask = [torch.tensor(x[\"anchor_attention_mask\"]).clone().detach() for x in features]\n",
    "        positive_mask = [torch.tensor(x[\"positive_attention_mask\"]).clone().detach() for x in features]\n",
    "        negative_mask = [torch.tensor(x[\"negative_attention_mask\"]).clone().detach() for x in features]\n",
    "        #text_mask = [torch.tensor(x[\"text_attention_mask\"]).clone().detach() for x in features]\n",
    "\n",
    "        anchor_ids = pad_sequence(anchor_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        positive_ids = pad_sequence(positive_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        negative_ids = pad_sequence(negative_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        #text_ids = pad_sequence(text_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "\n",
    "        anchor_mask = pad_sequence(anchor_mask, batch_first=True, padding_value=0)\n",
    "        positive_mask = pad_sequence(positive_mask, batch_first=True, padding_value=0)\n",
    "        negative_mask = pad_sequence(negative_mask, batch_first=True, padding_value=0)\n",
    "        #text_mask = pad_sequence(text_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "        labels = torch.tensor([x[\"labels\"] for x in features])\n",
    "\n",
    "        batch = {\n",
    "            \"input_ids\": [anchor_ids, positive_ids, negative_ids], #text_ids],\n",
    "            \"attention_mask\": [anchor_mask, positive_mask, negative_mask], #text_mask],\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "\n",
    "tri_data_collator = CustomDataCollator(tokenizer=tri_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/RoBERTa-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"FacebookAI/RoBERTa-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "base_model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "def triplet_loss(anchor_output, positive_output, negative_output, text_logits, labels):\n",
    "    text_targets = torch.tensor(labels).clone().detach().to(device)\n",
    "    classification_loss = nn.CrossEntropyLoss()(text_logits, text_targets)\n",
    "\n",
    "    triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "        margin=1.0, distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)\n",
    "    )\n",
    "    triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "    return classification_loss, triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    logits = logits[0]\n",
    "    predictions = np.argmax(logits, axis=-1).tolist()\n",
    "\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"recall\": recall, \"precision\": precision, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from models.models_rob import TripletModel\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10000,\n",
    "    learning_rate=5e-6, \n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=12, \n",
    "    logging_dir=\"./logs\",\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[],\n",
    "    optim=\"adafactor\",\n",
    ")\n",
    "\n",
    "tri_model = TripletModel(base_model, triplet_loss)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=tri_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tri_tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tri_tokenized_datasets[\"dev\"],\n",
    "    data_collator=tri_data_collator,\n",
    "    tokenizer=tri_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=tri_tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import os\n",
    "\n",
    "name = \"../trained_model/triplet_rob\"\n",
    "trainer.save_model(name)\n",
    "trainer.save_state()\n",
    "tri_model.save_pretrained(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ref': 'The FBI charged a Philadelphia woman on Thursday with trying to travel overseas to fight for ISIS. She\\'s one of three women arrested this week on terror charges. Two New York women were also taken into custody. An FBI complaint cites numerous social media messages dating back to August 2013 that were sent by Keonna Thomas, 30, also known as \"Young Lioness\" and \"Fatayat Al Khilafah.\" One Twitter message said, \"If we truly knew the realities ... we all would be rushing to join our brothers in the front lines pray ALLAH accept us as shuhada [martyrs].\" Another said, \"When you\\'re a mujahid [violent jihadi fighter] your death becomes a wedding.\" The FBI said Thomas purchased an electronic visa to Turkey on March 23. Turkey is known as the easiest place from which to enter Syria and join ISIS. An ISIS manual advises recruits to buy round-trip tickets to vacation spots such as Spain and then purchase tickets for their real destination once they arrive overseas, the FBI said. On March 26, Thomas purchased a ticket to Barcelona, with a March 29 departure and an April 15 return to the United States, the complaint said. It\\'s not clear when or where she was arrested. She was charged with knowingly attempting to provide material support and resources to a designated foreign terrorist organization. She could be sentenced to 15 years in prison. On Thursday, Noelle Velentzas, 28, and her former roommate, Asia Siddiqui, 31, were arrested in New York and accused of planning to build an explosive device for attacks in the United States, federal prosecutors said. In the past 18 months, the Justice Department\\'s National Security Division has prosecuted or is prosecuting more than 30 cases of people attempting to travel abroad to join or provide support to terrorist groups. Of those cases, 18 allegedly involve support to ISIS. \"The terrorist threat is more decentralized, more diffuse, more complicated,\" Homeland Security Secretary Jeh Johnson told reporters Thursday. \"It involves the potential lone wolf actor, it involves the effective use of social media, the Internet.\"\\n',\n",
       " 'text': 'Please judge the following statement whether it includes hallucination or not based on the Document above: The FBI has charged a Philadelphia woman, Keonna Thomas, with trying to travel overseas to fight for ISIS. This follows the arrests of two New York women, Noelle Velentzas and Asia Siddiqui, who were accused of planning to build an explosive device for attacks in the United States. Thomas purchased an electronic visa to Turkey and a round-trip ticket to Barcelona, following the advice given in an ISIS manual for recruits. She has been charged with attempting to provide material support to a terrorist organisation and could face 15 years in prison.',\n",
       " 'labels': 0,\n",
       " 'source': 'CNN/DM',\n",
       " 'model': 'gpt-4-0613',\n",
       " 'task_type': 'Summary',\n",
       " 'source_id': '15596'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you restart kernel, you need to load the model and tokenizer again.\n",
    "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
    "import torch\n",
    "from models.models_rob import TripletModel\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def triplet_loss(anchor_output, positive_output, negative_output, text_logits, labels):\n",
    "    text_targets = torch.tensor(labels).clone().detach().to(device)\n",
    "    classification_loss = nn.CrossEntropyLoss()(text_logits, text_targets)\n",
    "\n",
    "    triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n",
    "        margin=1.0, distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)\n",
    "    )\n",
    "    triplet_loss = triplet_loss_fn(anchor_output, positive_output, negative_output)\n",
    "    return classification_loss, triplet_loss\n",
    "\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\"FacebookAI/RoBERTa-base\")\n",
    "# load model and tokenizer\n",
    "name = \"../trained_models/triplet_rob\"\n",
    "tri_model = TripletModel.from_pretrained(base_model, triplet_loss, name)\n",
    "tri_tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "tri_model.to(device)\n",
    "\n",
    "\n",
    "def tri_tokenize_function(examples):\n",
    "    anchor = tri_tokenizer(examples[\"anchor\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    positive = tri_tokenizer(examples[\"positive\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    negative = tri_tokenizer(examples[\"negative\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    #text = tri_tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "\n",
    "    return {\n",
    "        \"anchor_input_ids\": anchor[\"input_ids\"],\n",
    "        \"anchor_attention_mask\": anchor[\"attention_mask\"],\n",
    "        \"positive_input_ids\": positive[\"input_ids\"],\n",
    "        \"positive_attention_mask\": positive[\"attention_mask\"],\n",
    "        \"negative_input_ids\": negative[\"input_ids\"],\n",
    "        \"negative_attention_mask\": negative[\"attention_mask\"],\n",
    "        #\"text_input_ids\": text[\"input_ids\"],\n",
    "        #\"text_attention_mask\": text[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "\n",
    "tri_tokenized_datasets = tri_raw_datasets.map(tri_tokenize_function, batched=True)\n",
    "tri_tokenized_datasets = tri_tokenized_datasets.remove_columns([\"anchor\", \"positive\", \"negative\", \"text\"])\n",
    "# tokenized_datasets.set_format(\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tri_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"../results_test.json\"):\n",
    "    with open(\"../results_test.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    data = []\n",
    "    for i, d in enumerate(test_data):\n",
    "        data.append({\"id\": i, \"label\": d[\"labels\"], \"task\": d[\"task_type\"]})\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "tri_model.eval()\n",
    "\n",
    "\n",
    "for i,d in tqdm(enumerate(tri_tokenized_datasets[\"test\"])):\n",
    "    anchor_input_ids = torch.tensor(d[\"anchor_input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    positive_input_ids = torch.tensor(d[\"positive_input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    negative_input_ids = torch.tensor(d[\"negative_input_ids\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    input_ids = [anchor_input_ids, positive_input_ids, negative_input_ids]\n",
    "    \n",
    "    anchor_attention_mask = torch.tensor(d[\"anchor_attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    positive_attention_mask = torch.tensor(d[\"positive_attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    negative_attention_mask = torch.tensor(d[\"negative_attention_mask\"], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    attention_mask = [anchor_attention_mask, positive_attention_mask, negative_attention_mask]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = tri_model(input_ids=input_ids, attention_mask=attention_mask, labels=[d[\"labels\"]])\n",
    "        logits = outputs.logits\n",
    "        predicted_index = torch.argmax(logits, dim=-1)\n",
    "    data[i][\"triplet_rob_logits\"] = logits.cpu().numpy()\n",
    "    data[i][\"triplet_rob_label\"] = predicted_index.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "true_labels = [item['label'] for item in data]\n",
    "predicted_labels = [item['triplet_rob_label'] for item in data]\n",
    "\n",
    "count = Counter(predicted_labels)\n",
    "print(count)\n",
    "\n",
    "accuracy_score(true_labels, predicted_labels), f1_score(true_labels, predicted_labels), precision_score(true_labels, predicted_labels), recall_score(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../results_test.json\", \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".acl2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
